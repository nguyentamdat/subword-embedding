{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from numpy import asarray, zeros\n",
    "from tensorflow.keras.layers import Embedding, Dense, Activation, Dropout, CuDNNLSTM\n",
    "from tensorflow.keras import Model, Input, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train_clean_set.csv')\n",
    "train_set = train_set[train_set['word_count'] <= 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>length</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>áo khoác adidas nu</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mỗi thùng bia đều có thẻ cào hả chị</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hàng mới về váy đũi nhún eo AMOUNT màu y hình ...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>3.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>polo adidas bayer AMOUNT cotton ạ size s âu dà...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>79</td>\n",
       "      <td>3.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adidas uk AMOUNT cân xanh size AMOUNT hồng siz...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>56</td>\n",
       "      <td>4.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         search_text  label  word_count  \\\n",
       "0                                 áo khoác adidas nu      1           4   \n",
       "1                mỗi thùng bia đều có thẻ cào hả chị      0           9   \n",
       "2  hàng mới về váy đũi nhún eo AMOUNT màu y hình ...      1          12   \n",
       "3  polo adidas bayer AMOUNT cotton ạ size s âu dà...      1          17   \n",
       "4  adidas uk AMOUNT cân xanh size AMOUNT hồng siz...      1          10   \n",
       "\n",
       "   length  avg_word  \n",
       "0      18  3.750000  \n",
       "1      35  3.000000  \n",
       "2      52  3.416667  \n",
       "3      79  3.705882  \n",
       "4      56  4.700000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>length</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>34085.000000</td>\n",
       "      <td>34085.000000</td>\n",
       "      <td>34085.000000</td>\n",
       "      <td>34085.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.477101</td>\n",
       "      <td>57.119349</td>\n",
       "      <td>262.530703</td>\n",
       "      <td>3.635064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499483</td>\n",
       "      <td>36.262552</td>\n",
       "      <td>168.013332</td>\n",
       "      <td>0.465427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>3.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>3.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>3.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>897.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label    word_count        length      avg_word\n",
       "count  34085.000000  34085.000000  34085.000000  34085.000000\n",
       "mean       0.477101     57.119349    262.530703      3.635064\n",
       "std        0.499483     36.262552    168.013332      0.465427\n",
       "min        0.000000      0.000000      0.000000      0.000000\n",
       "25%        0.000000     28.000000    128.000000      3.347826\n",
       "50%        0.000000     53.000000    242.000000      3.583333\n",
       "75%        1.000000     82.000000    373.000000      3.857143\n",
       "max        1.000000    150.000000    897.000000      9.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['áo', 'khoác', 'adidas', 'nu'])\n",
      " list(['mỗi', 'thùng', 'bia', 'đều', 'có', 'thẻ', 'cào', 'hả', 'chị'])\n",
      " list(['hàng', 'mới', 'về', 'váy', 'đũi', 'nhún', 'eo', 'AMOUNT', 'màu', 'y', 'hình', 'AMOUNT'])\n",
      " list(['polo', 'adidas', 'bayer', 'AMOUNT', 'cotton', 'ạ', 'size', 's', 'âu', 'dành', 'cho', 'ae', 'AMOUNT', 'ạ', 'giá', 'AMOUNT', 'sẵn'])\n",
      " list(['adidas', 'uk', 'AMOUNT', 'cân', 'xanh', 'size', 'AMOUNT', 'hồng', 'size', 'AMOUNT3y'])]\n"
     ]
    }
   ],
   "source": [
    "sentences = train_set['search_text'].apply(lambda x: str(x).split()).values\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 300\n",
    "\n",
    "w2v = Word2Vec(sentences, size=EMB_DIM, sg=1, window=1,min_count=5,iter=20,workers=multiprocessing.cpu_count(),sorted_vocab=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar:\n",
      " [('acer', 0.6284576058387756), ('mf839', 0.6206846237182617), ('mqd32', 0.583571195602417), ('️laptop', 0.5593997240066528), ('asus', 0.5572055578231812), ('lx', 0.5551203489303589), ('i5', 0.5485771298408508), ('ip7ip8', 0.5358346700668335), ('mtb', 0.5327499508857727), ('a6plusj8', 0.5314415693283081)]\n",
      "Vocabulary size: 8333\n"
     ]
    }
   ],
   "source": [
    "word_vectors = w2v.wv\n",
    "result = word_vectors.similar_by_word('dell')\n",
    "print(\"Most similar:\\n\", result[:10])\n",
    "words = list(w2v.wv.vocab)\n",
    "print('Vocabulary size: %d'%len(words))\n",
    "filename = 'embedding_word2vec.txt'\n",
    "word_vectors.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(filename):\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_matrix(embedding, vocab, MAX_WORDS):\n",
    "    weight_matrix = np.zeros((MAX_WORDS, 300))\n",
    "    num_loss = 0\n",
    "    for word, i in vocab.items():\n",
    "        if i < MAX_WORDS:\n",
    "            try:\n",
    "                weight_matrix[i] = embedding[word]\n",
    "            except:\n",
    "                num_loss += 1\n",
    "    print(\"Number of loss weight: {}\".format(num_loss))\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 8000\n",
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Tokenizer(num_words=MAX_WORDS, oov_token='UNK')\n",
    "vocab.fit_on_texts(train_set.search_text.astype(str))\n",
    "vocab_size = len(vocab.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29347\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loss weight: 182\n"
     ]
    }
   ],
   "source": [
    "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, vocab.word_index, MAX_WORDS)\n",
    "embedding_layer = Embedding(MAX_WORDS,300,weights=[embedding_vectors],input_length=MAX_LEN, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(shape=[MAX_LEN])\n",
    "    layer = embedding_layer(inputs)\n",
    "    layer = CuDNNLSTM(64, activity_regularizer=l1_l2(0,0))(layer)\n",
    "    layer = Dense(256)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1)(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 150)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 300)          2400000   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 64)                93696     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,510,593\n",
      "Trainable params: 2,510,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(optimizer=optimizers.RMSprop(), loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(vocab.texts_to_sequences(train_set.search_text.astype(str)),maxlen=MAX_LEN)\n",
    "Y_train = train_set.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "\n",
    "ES = callbacks.EarlyStopping(monitor='val_loss',min_delta=0.0001)\n",
    "CP = callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, monitor=\"val_acc\", mode=\"max\", save_best_only=True, verbose=1)\n",
    "\n",
    "cplist = [CP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27268 samples, validate on 6817 samples\n",
      "Epoch 1/20\n",
      "26752/27268 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.8500 - precision_2: 0.8361 - recall_2: 0.8517\n",
      "Epoch 00001: val_acc improved from -inf to 0.86328, saving model to training/cp.ckpt\n",
      "27268/27268 [==============================] - 4s 131us/sample - loss: 0.3479 - acc: 0.8513 - precision_2: 0.8374 - recall_2: 0.8535 - val_loss: 0.3224 - val_acc: 0.8633 - val_precision_2: 0.9430 - val_recall_2: 0.7618\n",
      "Epoch 2/20\n",
      "26880/27268 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9196 - precision_2: 0.9144 - recall_2: 0.9170\n",
      "Epoch 00002: val_acc improved from 0.86328 to 0.93091, saving model to training/cp.ckpt\n",
      "27268/27268 [==============================] - 3s 109us/sample - loss: 0.2105 - acc: 0.9195 - precision_2: 0.9143 - recall_2: 0.9170 - val_loss: 0.1870 - val_acc: 0.9309 - val_precision_2: 0.9209 - val_recall_2: 0.9369\n",
      "Epoch 3/20\n",
      "27008/27268 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9429 - precision_2: 0.9369 - recall_2: 0.9437\n",
      "Epoch 00003: val_acc improved from 0.93091 to 0.94088, saving model to training/cp.ckpt\n",
      "27268/27268 [==============================] - 3s 110us/sample - loss: 0.1550 - acc: 0.9429 - precision_2: 0.9372 - recall_2: 0.9433 - val_loss: 0.1775 - val_acc: 0.9409 - val_precision_2: 0.9350 - val_recall_2: 0.9427\n",
      "Epoch 4/20\n",
      "27264/27268 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9571 - precision_2: 0.9561 - recall_2: 0.9537\n",
      "Epoch 00004: val_acc did not improve from 0.94088\n",
      "27268/27268 [==============================] - 3s 106us/sample - loss: 0.1175 - acc: 0.9571 - precision_2: 0.9561 - recall_2: 0.9537 - val_loss: 0.8361 - val_acc: 0.7565 - val_precision_2: 0.6675 - val_recall_2: 0.9838\n",
      "Epoch 5/20\n",
      "27136/27268 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9679 - precision_2: 0.9643 - recall_2: 0.9686\n",
      "Epoch 00005: val_acc did not improve from 0.94088\n",
      "27268/27268 [==============================] - 3s 106us/sample - loss: 0.0907 - acc: 0.9680 - precision_2: 0.9643 - recall_2: 0.9687 - val_loss: 0.2340 - val_acc: 0.9396 - val_precision_2: 0.9656 - val_recall_2: 0.9067\n",
      "Epoch 6/20\n",
      "26752/27268 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9778 - precision_2: 0.9776 - recall_2: 0.9756\n",
      "Epoch 00006: val_acc did not improve from 0.94088\n",
      "27268/27268 [==============================] - 3s 106us/sample - loss: 0.0664 - acc: 0.9776 - precision_2: 0.9774 - recall_2: 0.9755 - val_loss: 0.1763 - val_acc: 0.9382 - val_precision_2: 0.9580 - val_recall_2: 0.9116\n",
      "Epoch 7/20\n",
      "27008/27268 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9836 - precision_2: 0.9827 - recall_2: 0.9828\n",
      "Epoch 00007: val_acc improved from 0.94088 to 0.94264, saving model to training/cp.ckpt\n",
      "27268/27268 [==============================] - 3s 111us/sample - loss: 0.0475 - acc: 0.9836 - precision_2: 0.9827 - recall_2: 0.9829 - val_loss: 0.2113 - val_acc: 0.9426 - val_precision_2: 0.9429 - val_recall_2: 0.9375\n",
      "Epoch 8/20\n",
      "27008/27268 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9882 - precision_2: 0.9879 - recall_2: 0.9872\n",
      "Epoch 00008: val_acc improved from 0.94264 to 0.94587, saving model to training/cp.ckpt\n",
      "27268/27268 [==============================] - 3s 112us/sample - loss: 0.0351 - acc: 0.9882 - precision_2: 0.9879 - recall_2: 0.9872 - val_loss: 0.2065 - val_acc: 0.9459 - val_precision_2: 0.9412 - val_recall_2: 0.9466\n",
      "Epoch 9/20\n",
      "26880/27268 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9915 - precision_2: 0.9910 - recall_2: 0.9912\n",
      "Epoch 00009: val_acc did not improve from 0.94587\n",
      "27268/27268 [==============================] - 3s 105us/sample - loss: 0.0251 - acc: 0.9914 - precision_2: 0.9911 - recall_2: 0.9909 - val_loss: 0.2405 - val_acc: 0.9388 - val_precision_2: 0.9297 - val_recall_2: 0.9442\n",
      "Epoch 10/20\n",
      "27008/27268 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9945 - precision_2: 0.9946 - recall_2: 0.9939\n",
      "Epoch 00010: val_acc did not improve from 0.94587\n",
      "27268/27268 [==============================] - 3s 105us/sample - loss: 0.0172 - acc: 0.9945 - precision_2: 0.9946 - recall_2: 0.9938 - val_loss: 0.2779 - val_acc: 0.9429 - val_precision_2: 0.9584 - val_recall_2: 0.9213\n",
      "Epoch 11/20\n",
      "27136/27268 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9962 - precision_2: 0.9967 - recall_2: 0.9953\n",
      "Epoch 00011: val_acc improved from 0.94587 to 0.94836, saving model to training/cp.ckpt\n",
      "27268/27268 [==============================] - 3s 115us/sample - loss: 0.0130 - acc: 0.9962 - precision_2: 0.9967 - recall_2: 0.9953 - val_loss: 0.3289 - val_acc: 0.9484 - val_precision_2: 0.9404 - val_recall_2: 0.9530\n",
      "Epoch 12/20\n",
      "26880/27268 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9965 - precision_2: 0.9969 - recall_2: 0.9958\n",
      "Epoch 00012: val_acc did not improve from 0.94836\n",
      "27268/27268 [==============================] - 3s 103us/sample - loss: 0.0111 - acc: 0.9966 - precision_2: 0.9970 - recall_2: 0.9958 - val_loss: 0.3233 - val_acc: 0.9478 - val_precision_2: 0.9438 - val_recall_2: 0.9478\n",
      "Epoch 13/20\n",
      "26752/27268 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9980 - precision_2: 0.9979 - recall_2: 0.9979\n",
      "Epoch 00013: val_acc did not improve from 0.94836\n",
      "27268/27268 [==============================] - 3s 105us/sample - loss: 0.0087 - acc: 0.9977 - precision_2: 0.9976 - recall_2: 0.9975 - val_loss: 0.3116 - val_acc: 0.9394 - val_precision_2: 0.9532 - val_recall_2: 0.9192\n",
      "Epoch 14/20\n",
      "27136/27268 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9981 - precision_2: 0.9985 - recall_2: 0.9976\n",
      "Epoch 00014: val_acc did not improve from 0.94836\n",
      "27268/27268 [==============================] - 3s 105us/sample - loss: 0.0074 - acc: 0.9981 - precision_2: 0.9985 - recall_2: 0.9976 - val_loss: 0.3501 - val_acc: 0.9479 - val_precision_2: 0.9430 - val_recall_2: 0.9491\n",
      "Epoch 15/20\n",
      "27136/27268 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9980 - precision_2: 0.9978 - recall_2: 0.9981\n",
      "Epoch 00015: val_acc did not improve from 0.94836\n",
      "27268/27268 [==============================] - 3s 107us/sample - loss: 0.0071 - acc: 0.9981 - precision_2: 0.9978 - recall_2: 0.9981 - val_loss: 0.4016 - val_acc: 0.9457 - val_precision_2: 0.9409 - val_recall_2: 0.9466\n",
      "Epoch 16/20\n",
      "27264/27268 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9983 - precision_2: 0.9983 - recall_2: 0.9981\n",
      "Epoch 00016: val_acc did not improve from 0.94836\n",
      "27268/27268 [==============================] - 3s 108us/sample - loss: 0.0061 - acc: 0.9983 - precision_2: 0.9983 - recall_2: 0.9981 - val_loss: 0.3958 - val_acc: 0.9446 - val_precision_2: 0.9329 - val_recall_2: 0.9533\n",
      "Epoch 17/20\n",
      "27136/27268 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9984 - precision_2: 0.9981 - recall_2: 0.9985\n",
      "Epoch 00017: val_acc did not improve from 0.94836\n",
      "27268/27268 [==============================] - 3s 105us/sample - loss: 0.0051 - acc: 0.9984 - precision_2: 0.9981 - recall_2: 0.9985 - val_loss: 0.3961 - val_acc: 0.9429 - val_precision_2: 0.9290 - val_recall_2: 0.9543\n",
      "Epoch 18/20\n",
      "26880/27268 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9987 - precision_2: 0.9990 - recall_2: 0.9984\n",
      "Epoch 00018: val_acc did not improve from 0.94836\n",
      "27268/27268 [==============================] - 3s 105us/sample - loss: 0.0045 - acc: 0.9988 - precision_2: 0.9990 - recall_2: 0.9984 - val_loss: 0.4346 - val_acc: 0.9462 - val_precision_2: 0.9472 - val_recall_2: 0.9405\n",
      "Epoch 19/20\n",
      "26752/27268 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9989 - precision_2: 0.9989 - recall_2: 0.9988\n",
      "Epoch 00019: val_acc did not improve from 0.94836\n",
      "27268/27268 [==============================] - 3s 106us/sample - loss: 0.0042 - acc: 0.9989 - precision_2: 0.9989 - recall_2: 0.9988 - val_loss: 0.4769 - val_acc: 0.9437 - val_precision_2: 0.9385 - val_recall_2: 0.9448\n",
      "Epoch 20/20\n",
      "26880/27268 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9988 - precision_2: 0.9985 - recall_2: 0.9991\n",
      "Epoch 00020: val_acc did not improve from 0.94836\n",
      "27268/27268 [==============================] - 3s 105us/sample - loss: 0.0045 - acc: 0.9989 - precision_2: 0.9985 - recall_2: 0.9991 - val_loss: 0.4658 - val_acc: 0.9441 - val_precision_2: 0.9448 - val_recall_2: 0.9387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb4f9eda908>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=128, epochs=20, validation_split=0.2, callbacks=cplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t     cp.ckpt.data-00001-of-00002\r\n",
      "cp.ckpt.data-00000-of-00002  cp.ckpt.index\r\n"
     ]
    }
   ],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('test_clean_set.csv')\n",
    "X_test = test_dataset.search_text.astype(str)\n",
    "Y_test = test_dataset.label\n",
    "X_test_matrix = pad_sequences(vocab.texts_to_sequences(X_test), maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 88us/sample - loss: 0.6797 - acc: 0.9228 - precision_2: 0.9234 - recall_2: 0.9204\n",
      "Test set\n",
      "  Loss: 0.680\n",
      "  Accuracy: 0.923\n",
      "  Recall: 0.923\n",
      "  Precision: 0.920\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test_matrix, Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Recall: {:0.3f}\\n  Precision: {:0.3f}'.format(accr[0],accr[1],accr[2],accr[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9222    0.9251    0.9237      5049\n",
      "           1     0.9234    0.9204    0.9219      4951\n",
      "\n",
      "    accuracy                         0.9228     10000\n",
      "   macro avg     0.9228    0.9228    0.9228     10000\n",
      "weighted avg     0.9228    0.9228    0.9228     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_predicted = model.predict(X_test_matrix).transpose()[0].round()\n",
    "print(classification_report(Y_test.values, Y_predicted, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9306    0.9220    0.9263      5049\n",
      "           1     0.9212    0.9299    0.9255      4951\n",
      "\n",
      "    accuracy                         0.9259     10000\n",
      "   macro avg     0.9259    0.9259    0.9259     10000\n",
      "weighted avg     0.9259    0.9259    0.9259     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "Y_predicted = model.predict(X_test_matrix).transpose()[0].round()\n",
    "print(classification_report(Y_test.values, Y_predicted, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jnb] *",
   "language": "python",
   "name": "conda-env-jnb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
